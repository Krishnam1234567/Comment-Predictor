{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "import evaluate\n",
    "from transformers import (\n",
    "    BartForConditionalGeneration,\n",
    "    BartTokenizer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    T5Tokenizer,\n",
    "    T5ForConditionalGeneration\n",
    ")\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "id": "6fbd1f168ad6361d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df = pd.read_csv(\"../data/processed_data.csv\", index_col=[0])\n",
    "df.index = pd.to_datetime(df.index)"
   ],
   "id": "b8526de972656f2e",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "model_name = \"t5-small\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df = df.sort_values(by='datetime')\n",
    "split_index = int(len(df) * 0.9)\n",
    "df_train_time = df.iloc[:split_index]\n",
    "df_val_time = df.iloc[split_index:]\n",
    "print(f\"\\n--- Time-based Split ---\")\n",
    "print(f\"Total samples: {len(df)}\")\n",
    "print(f\"Training samples (90%): {len(df_train_time)}\")\n",
    "print(f\"Validation samples (10%): {len(df_val_time)}\")\n",
    "print(f\"Training data goes up to: {df_train_time['datetime'].max()}\")\n",
    "print(f\"Validation data starts from: {df_val_time['datetime'].min()}\")"
   ],
   "id": "b87dff46c41b49a8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "all_companies = df['inferred company'].unique()\n",
    "train_companies, val_companies = train_test_split(all_companies, test_size=0.1, random_state=42)\n",
    "df_train_brands = df[df['inferred company'].isin(train_companies)]\n",
    "df_val_brands = df[df['inferred company'].isin(val_companies)]\n",
    "print(f\"Companies for training: {len(train_companies)}\")\n",
    "print(f\"Companies for validation: {len(val_companies)}\")\n",
    "print(f\"Training samples: {len(df_train_brands)}\")\n",
    "print(f\"Validation samples: {len(df_val_brands)}\")"
   ],
   "id": "7e9c6faebc9dd259",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_dataset = Dataset.from_pandas(df_train_time[['prompt', 'content']])\n",
    "val_dataset = Dataset.from_pandas(df_val_time[['prompt', 'content']])"
   ],
   "id": "f1556b5f2e6bd670",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def tokenize_function(examples):\n",
    "\n",
    "    model_inputs = tokenizer(\n",
    "        examples['prompt'],\n",
    "        max_length=128,  # Max length for the input prompt\n",
    "        padding=\"max_length\",\n",
    "        truncation=True\n",
    "    )\n",
    "\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            examples['content'],\n",
    "            max_length=64,   # Max length for the output tweet\n",
    "            padding=\"max_length\",\n",
    "            truncation=True\n",
    "        )\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ],
   "id": "a6804b1c84ba4325",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"\\n--- Tokenizing datasets... ---\")\n",
    "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_train_dataset = tokenized_train_dataset.remove_columns(['prompt', 'content'])\n",
    "tokenized_val_dataset = tokenized_val_dataset.remove_columns(['prompt', 'content'])\n",
    "print(tokenized_train_dataset[0])"
   ],
   "id": "4f1dfe1ae1b04e73",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "metric_rouge = evaluate.load(\"rouge\")\n",
    "metric_bleu = evaluate.load(\"bleu\")"
   ],
   "id": "26e67338fe697c05",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "\n",
    "    # Decode the predicted tokens back into text\n",
    "    # skip_special_tokens=True removes padding/control tokens\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    # Replace -100 in labels (which are padding) with the pad token\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    # Decode the label tokens back into text\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # ROUGE metrics\n",
    "    rouge_result = metric_rouge.compute(\n",
    "        predictions=decoded_preds,\n",
    "        references=decoded_labels,\n",
    "        use_stemmer=True\n",
    "    )\n",
    "\n",
    "    # BLEU metrics\n",
    "    # For BLEU, references must be a list of lists\n",
    "    decoded_labels_list = [[label] for label in decoded_labels]\n",
    "    bleu_result = metric_bleu.compute(\n",
    "        predictions=decoded_preds,\n",
    "        references=decoded_labels_list\n",
    "    )\n",
    "\n",
    "    # Combine them and return\n",
    "    # We'll just pick a few key metrics to log\n",
    "    result = {\n",
    "        \"rouge1\": rouge_result[\"rouge1\"],\n",
    "        \"rouge2\": rouge_result[\"rouge2\"],\n",
    "        \"rougeL\": rouge_result[\"rougeL\"],\n",
    "        \"bleu\": bleu_result[\"bleu\"]\n",
    "    }\n",
    "\n",
    "    return result"
   ],
   "id": "adbb40061424413",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model_name = \"facebook/bart-base\"\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# --- 2. Data Collator ---\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "\n",
    "# --- 3. Metric Function ---\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "\n",
    "    # Decode predictions and labels\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "\n",
    "    # Replace -100 in labels as we can't decode them\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Compute ROUGE scores\n",
    "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "\n",
    "    # Extract f-measure for ROUGE\n",
    "    result = {k: round(v.mid.fmeasure * 100, 2) for k, v in result.items()}\n",
    "\n",
    "    # Add a key for the metric-for-best-model\n",
    "    result[\"rougeL\"] = result.get(\"rougeL\", 0.0)\n",
    "    return result\n",
    "\n",
    "# --- 4. Training Arguments ---\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results_bart\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs_bart\",\n",
    "    save_steps=1000,\n",
    "    logging_steps=100,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"rougeL\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_total_limit=2,\n",
    "    fp16=True,\n",
    "    report_to=\"tensorboard\",\n",
    ")\n",
    "\n",
    "# --- 5. Trainer ---\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,  # You must have this variable defined\n",
    "    eval_dataset=tokenized_val_dataset,    # You must have this variable defined\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# --- 6. Train ---\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "print(\"Training finished.\")\n",
    "\n",
    "# --- 7. Save Model ---\n",
    "print(\"Saving final model...\")\n",
    "trainer.save_model(\"../results_bart/final_model\")\n",
    "tokenizer.save_pretrained(\"../results_bart/final_model\")\n",
    "print(\"Model saved to results_bart/final_model\")"
   ],
   "id": "ecc64db6cdd5be1d",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
